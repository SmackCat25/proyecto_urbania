{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## urllib.robotparser: Interpretación de robots.txt en Python\n",
    "\n",
    "La clase RobotFileParser, que forma parte de la librería estándar urllib, se utiliza para interpretar el contenido de los archivos robots.txt de manera programática.\n",
    "\n",
    "Su objetivo principal es determinar si un user-agent específico tiene permiso para acceder a una URL de un sitio web, basándose en las directivas Allow y Disallow que contiene el archivo.\n",
    "\n",
    "El flujo de trabajo incluye tres métodos esenciales:\n",
    "\n",
    "set_url(url): Establece la ubicación del archivo robots.txt que se va a analizar.\n",
    "\n",
    "read(): Realiza la descarga y el procesamiento del contenido del archivo. El parser carga las reglas en memoria para poder consultarlas.\n",
    "\n",
    "can_fetch(useragent, url_path): Consulta las reglas ya cargadas para determinar si el useragent indicado tiene permiso para acceder a la url_path. Devuelve un booleano: True si el acceso está permitido, False si está denegado.\n",
    "\n",
    "Para una referencia técnica completa de todos sus métodos y comportamiento, consulte la documentación oficial:\n",
    "https://docs.python.org/3/library/urllib.robotparser.html"
   ],
   "id": "641dcca35d76bd33"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-27T01:56:02.564292Z",
     "start_time": "2025-08-27T01:56:02.155420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "robots_url = 'https://repositorio.pucp.edu.pe/robots.txt'\n",
    "parser = RobotFileParser()\n",
    "parser.set_url(robots_url)\n",
    "parser.read()\n"
   ],
   "id": "6d0ee19b6e77ca9d",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Definición del User-Agent\n",
    "\n",
    "Se asigna un nombre único para identificar a nuestro bot. Es una práctica estándar que permite a los sitios web reconocer el tráfico automatizado."
   ],
   "id": "537f22b300936b54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T01:58:44.988795Z",
     "start_time": "2025-08-27T01:58:44.985083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_agent = 'MineriaWeb-2025-2/1.0'\n",
    "parser.can_fetch(user_agent, \"https://repositorio.pucp.edu.pe/collections/a4e60678-5b9d-4a5d-be1e-0b8aef6b566a\")"
   ],
   "id": "f767fcf199cb61a7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "61b3e2bac8f3d1cb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
